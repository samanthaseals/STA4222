---
title: "Simple Probability Samples"
subtitle: "Chapter 2"
author: "Dr. Seals"
format: 
  revealjs:
    theme: uwf
    self-contained: false
    slide-number: false
    footer: "[STA4222 - Sampling Theory](https://samanthaseals.github.io/STA4222)"
    width: 1600
    height: 900
    chalkboard: true
    df-print: paged
    html-math-method: katex
editor: source
---

## 2.1: Types of Probability Samples

-   **Simple random sample**: a simple random sample of size *n* is taken when every possible subset of *n* units in the population has equal probability of being the sample chosen.

    -   i.e., every unit has equal probability of being selected for the sample

-   **Stratified random sample**: the population is divided into subgroups (strata) and a simple random sample is selected from each stratum.

    -   Strata are often (1) subgroups of interest, (2) easily identified.

-   **Cluster sample**: observation units are grouped into larger sampling units called clusters; a simple random sample of the clusters is then chosen.

    -   This is different from stratified random sampling because we are sampling the *entire cluster*.

-   **Systematic sample**: a starting point is sampled from a list of population members using a random number, then, every *k*^th^ unit thereafter is chosen to be in the sample.

    -   The sample consists of units equally spaced in the list.

## 2.1: Types of Probability Samples

<center><img src="images/L02a.png" width="1000"/></center>

## 2.2: Framework for Probability Sampling

-   Let there be *N* units in the finite population. We denote the finite population (or universe) with

$$
\mathscr{U} = \{ 1, \ 2, \ ... \ , N\}
$$

-   When we choose a sample, it is a subset of $\mathscr{U}$. We denote this as $\mathscr{S}$.

-   Suppose the population has four units, $\mathscr{U} = \{1, \ 2, \ 3, \ 4\}$. The following are possible samples, 
$$
    \begin{align*}
    \mathscr{S}_1 &= \{1, \ 2\} \ \ \ \ \ \mathscr{S}_2 = \{1, \ 3 \} \ \ \ \ \ \mathscr{S}_3 = \{1, \ 4 \} \\
    \mathscr{S}_4 &= \{2, \ 3 \} \ \ \ \ \ \mathscr{S}_5 = \{2, \ 4 \} \ \ \ \ \ \mathscr{S}_6 = \{3, \ 4 \}
    \end{align*}
$$

## 2.2: Framework for Probability Sampling

-   In probability sampling, each possible sample $\mathscr{S}$ has a known probability, $\text{P}[\mathscr{S}]$, of being chosen.

    -   Remember, any probability must be between 0 and 1.

        -   i.e., $0 \le \text{P}[\mathscr{S}] \le 1$

    -   The probabilities of all possible samples must sum to 1.

-   Thus, each unit has a known probability of being selected for a sample,

$$ 
\begin{align*}
\pi_i &= \text{P}[\text{unit $i$ in sample}] \\
&= \sum \text{probabilities of all possible samples that contain unit $i$}
\end{align*}
$$

## 2.2: Framework for Probability Sampling

-   Let's revisit the *n* = 2 samples for $\mathscr{U} = \{1, \ 2, \ 3, \ 4\}$.

$$
\begin{align*}
\mathscr{S}_1 &= \{1, \ 2\} \ \ \ \ \ \mathscr{S}_2 = \{1, \ 3 \} \ \ \ \ \ \mathscr{S}_3 = \{1, \ 4 \} \\
\mathscr{S}_4 &= \{2, \ 3 \} \ \ \ \ \ \mathscr{S}_5 = \{2, \ 4 \} \ \ \ \ \ \mathscr{S}_6 = \{3, \ 4 \}
\end{align*}
$$

-   Suppose they have probabilities,

$$
\begin{align*}
\text{P}[\mathscr{S}_1] &= 1/3  & \text{P}[\mathscr{S}_2] &= 1/6  & \text{P}[\mathscr{S}_3] &= 1/2 \\
\text{P}[\mathscr{S}_4] &= 0  & \text{P}[\mathscr{S}_5] &= 0  & \text{P}[\mathscr{S}_6] &= 0
\end{align*}
$$

-   Find $\pi_i$.

## 2.2: Framework for Probability Sampling

-   If the sample probabilities are as given,

$$
\begin{align*}
\text{P}[\mathscr{S}_1= \{1, \ 2\}] &= 1/3  & \text{P}[\mathscr{S}_2= \{1, \ 3\}] &= 1/6  & \text{P}[\mathscr{S}_3= \{1, \ 4\}] &= 1/2 \\
\text{P}[\mathscr{S}_4= \{2, \ 3\}] &= 0  & \text{P}[\mathscr{S}_5= \{2, \ 4\}] &= 0  & \text{P}[\mathscr{S}_6= \{3, \ 4\}] &= 0
\end{align*}
$$

-   Then, the unit probabilities are as follows,

$$
\begin{align*}
\pi_1 &= \text{P}[\mathscr{S}_1] + \text{P}[\mathscr{S}_2] + \text{P}[\mathscr{S}_3] = 1/2 \\
\pi_2 &= \text{P}[\mathscr{S}_1] + \text{P}[\mathscr{S}_4] + \text{P}[\mathscr{S}_5] = 1/3 \\
\pi_3 &= \text{P}[\mathscr{S}_2] + \text{P}[\mathscr{S}_4] + \text{P}[\mathscr{S}_6] = 2/3 \\
\pi_4 &= \text{P}[\mathscr{S}_3] + \text{P}[\mathscr{S}_5] + \text{P}[\mathscr{S}_6] = 1/2 
\end{align*}
$$

## 2.2: Framework for Probability Sampling

-   Consider a new population with *N* = 8,

$$
\mathscr{U} = \{1, \ 2, \ 3, \ 4, \ 5, \ 6, \ 7, \ 8 \}
$$

-   The values of $y_i$ are

$$ 
\begin{align*}
y_1 &= 1 & y_2 &= 2 & y_3 &= 4 & y_4 &= 4 \\
y_5 &= 7 & y_6 &= 7 & y_7 &= 7 & y_8 &= 8
\end{align*}
$$

-   When drawing samples of size *n* = 4, there are 70 possible samples ðŸ˜±

    -   Thankfully the textbook author has given us a spreadsheet with all possible samples! ðŸ¤“

## 2.2: Framework for Probability Sampling

-   Because we are using R, we will install the [`SDAResources` package](https://cran.r-project.org/web/packages/SDAResources/index.html)

```{r, eval = FALSE, echo = TRUE}
install.packages("SDAResources")
```

-   Then, we will call in the package,

```{r, echo = TRUE}
library(SDAResources)
```

-   Note 1: After installing a package, remove the `install.packages()` call.

    -   When repeatedly running `install.packages()`, we are repeatedly downloading and installing packages...

-   Note 2: In the event that you forget to remove an `install.packages()` in your Quarto document, that's why it will not compile.

-   Note 3: No, I do not know why we use quotes in `install.packages()` and no quotes in `library()`.

## 2.2: Framework for Probability Sampling

-   Now, we can call in (and preview) the dataset.

-   The last column (*total*) contains the total (sum of all observations in the sample).

```{r, echo = TRUE}
data(sample70) # call in data from library
head(sample70, n = 5) # show the first 5 observations
```

## 2.2: Framework for Probability Sampling

-   Let's consider estimating a population total, $\tau$.

$$
\tau = \sum_{i=1}^N y_i
$$

-   We can estimate $\tau$ using the sample mean,

$$
\hat{\tau}_{\mathscr{S}} = N \bar{y}_{\mathscr{S}}
$$

-   **Sampling distribution**: the distribution of different values of the statistic obtained by the process of taking all possible samples from the population.

    -   Because we have all possible samples and their totals for $\mathscr{U} = \{1, \ 2, \ 3, \ 4, \ 5, \ 6, \ 7, \ 8 \}$, we can calculate the sampling distribution for $\hat{\tau}$ under samples of size *n* = 4.

    -   As *N* grows larger, it becomes harder/more cumbersome to enumerate all possible samples.

    -   We can only do this if we know all values in the population.

## 2.2: Framework for Probability Sampling

```{r, echo = TRUE}
library(tidyverse) # call in tidyverse package 
counts <- sample70 %>% # create dataset called "counts"
  count(total) %>% # create frequency distribution of tau hat
  mutate(prob = n/70) # create column with probability of tau hat
head(counts, n=5) # show first 5 observations
```

## 2.2: Framework for Probability Sampling

-   **Expected value**: what is expected; an average

$$
\text{E}[X] = \sum_{i=1}^n x_i \text{P}[x_i]
$$

-   We can find the expected value of $\hat{\tau}$,

$$
\text{E}\left[\hat{\tau}\right] = \sum_{i=1}^n \hat{\tau}_i \text{P}\left[\hat{\tau}_i\right]
$$

-   In our working example, $\text{E}\left[ \hat{\tau} \right]$ = 40.

```{r, echo = TRUE}
counts <- counts %>% mutate(expect = total*prob) # find tau hat * P[tau hat]
sum(counts$expect) # find E[tau hat]
```

## 2.2: Framework for Probability Sampling

-   **Bias**: estimation bias is the difference between the expected value and the true value of the parameter.

$$
\text{bias}\left[ \hat{\theta} \right] = \text{E} \left[ \hat{\theta} \right] - \theta
$$

-   In the case of $\hat{\tau}$,

$$
\text{bias}\left[ \hat{\tau} \right] = \text{E} \left[ \hat{\tau} \right] - \tau
$$

-   In our working example, $\tau$ = `r 1+2+4+4+7+7+7+8` and $\text{E}\left[ \hat{\tau} \right]$ = 40.

    -   Thus, $\hat{\tau}$ is unbiased for $\tau$.

        -   This is true regardless of the population we are dealing with.

## 2.2: Framework for Probability Sampling

-   We can find the variance of an estimator,

$$
\text{var}[X] = \text{E}\left[ (X - \text{E}[X])^2  \right]  = \sum_{i=1}^n \text{P}[x_i]\left(x_i -\text{E}\left[x_i\right]\right)^2
$$

-   Applying this to $\hat{\tau}$,

$$
\text{var}[\hat{\tau}] = \text{E}\left[ (\hat{\tau} - \text{E}\left[\hat{\tau}\right])^2  \right] = \sum_{i=1}^n \text{P}[\hat{\tau}_i]\left(\hat{\tau} - \text{E}\left[\hat{\tau}\right]\right)^2
$$

-   In our working example, $\text{var}\left[ \hat{\tau} \right]$ = 54.86.

```{r, echo = TRUE}
counts <- counts %>% mutate(var = prob*(total-40)^2) # find terms for summation
round(sum(counts$var), 2) # find var[tau hat] & round to 2 decimal places
```

## 2.2: Framework for Probability Sampling

-   While unbiased estimators are nice, they are not always available.

-   We look to the mean squared error (MSE) to measure the accuracy of an estimator,

$$
\text{MSE}\left[ \hat{\tau} \right] = \text{E}\left[ \left( \hat{\tau} - \tau \right)^2 \right] = \text{var}\left[ \hat{\tau} \right] + \left( \text{bias}\left[ \hat{\tau} \right] \right)^2
$$

-   Estimators can be unbiased (A), precise but not unbiased (B), or accurate (C).

<center><img src="images/L02b.png"></center>

## 2.3: Simple Random Sampling

- This is the simplest of probability sampling and the first building block for all concepts in this course. 

- Simple random sample *with replacement*: a sampling unit may be selected more than once.

- Simple random sample *without replacement*: a sampling unit may not be selected more than once.

- When dealing with finite populations, sampling the same unit twice does not give additional information.

    - For this reason, we most often choose to sample without replacement.

## 2.3: Simple Random Sampling

- When drawing a sample of size *n* from a population of size *N* without replacement:

    - There are <sub>N</sub>C<sub>n</sub> possible samples.
    
    - The probability of selecting any individual sample $\mathscr{S}$ of *n* units is
    
$$
\text{P}\left[ \mathscr{S} \right] = \frac{1}{_N\text{C}_n} = \frac{n!(N-n)!}{N!}
$$

- Under this definition, it can be shown that $\pi_i=n/N$.

## 2.3: Simple Random Sampling

- Let us consider the **agpop** data, provided by the textbook.

    - This gives historical data about farms in the *N* = 3078 counties in the US for 1982, 1987, and 1997.
    
```{r, echo = TRUE}    
data(agpop)
head(agpop, n=5)
```

## 2.3: Simple Random Sampling

- We will use the [`slice_sample()` function](https://dplyr.tidyverse.org/reference/slice.html) from the [`dplyr` package](https://dplyr.tidyverse.org/index.html) (included in [`tidyverse`](https://www.tidyverse.org/)).

```{r, echo = TRUE}
agsamp <- agpop %>% slice_sample(n=5)
head(agsamp)
```

## 2.3: Simple Random Sampling

- Every time we run [`slice_sample()`](https://dplyr.tidyverse.org/reference/slice.html), it will produce a different sample.

```{r, echo = TRUE}
agsamp <- agpop %>% slice_sample(n=5)
head(agsamp)
```

## 2.3: Simple Random Sampling

- A random number generator (RNG) is used to determine a starting point for the sampling.

    - The default RNG in R (and is considered the "best" RNG) is the [Mersenne-Twister](https://en.wikipedia.org/wiki/Mersenne_Twister).
    
- For reproducibility purposes, we can specify a starting point (called a "seed") using the [`set.seed()` function](https://r-coder.com/set-seed-r/).

    - Note that the seed does not "stick" in the system.

```{r, echo = TRUE}
set.seed(528)
agsamp <- agpop %>% slice_sample(n=5)
head(agsamp)
```

## 2.3: Simple Random Sampling

- Let's run it again to verify that it reproduces the sample we saw on the last slide.

    - We should see Scott, Ashland, Dickinson, Green, and Garza

```{r, echo = TRUE}
set.seed(528)
agsamp <- agpop %>% slice_sample(n=5)
head(agsamp)
```

## 2.3: Simple Random Sampling

- Let's now take a sample of 300 to be our working example.

```{r, echo = TRUE}
set.seed(9185)
agsamp <- agpop %>% slice_sample(n=300)
head(agsamp, n=5)
```

## 2.3: Simple Random Sampling

- Let's first visualize our data using a histogram of the number of acres devoted to farms in 1992 (*acres92*).

<center>
```{r, echo = TRUE, warning = FALSE}
agsamp %>% ggplot(aes(x = acres92)) + # pipe data into ggplot(), specify x-axis
  geom_histogram(bins = 70) + # request histogram with 70 bins
  labs(x = "Acres", y = "Number of Farms") + # change axis labels
  theme_bw() # change theme of graph
```
</center>

## 2.3: Simple Random Sampling

<center>
```{r, warning = FALSE}
agsamp %>% ggplot(aes(x = acres92)) + # pipe data into ggplot(), specify x-axis
  geom_histogram(bins = 70) + # request histogram with 70 bins
  labs(x = "Acres", y = "Number of Farms") + # change axis labels
  theme_bw() # change theme of graph
```
</center>

- Most counties have fewer than 300,000 acres in farms.

    - One county has over 700,000 acres in farms!
    
- Even without the obvious outlier, the number of acres is right-skewed.

## 2.3: Simple Random Sampling

- **Mean**: $\bar{y}$ is used to estimate $\mu$

$$
\bar{y} = \frac{1}{n} \sum_{i \in \mathscr{S}} y_i = \frac{1}{n} \sum_{i =1}^n y_i
$$

- **Variance of the estimated mean**: measures the variability among estimates of $\mu$ from different samples

$$
\hat{\text{var}} \left[ \bar{y} \right] = \frac{s^2}{n} \left(1 - \frac{n}{N}\right), 
$$

- Note that $s^2$ is the variance of the sample,

$$
S^2 = \frac{1}{n-1} \sum_{i=1}^n \left( y_i - \bar{y} \right)^2
$$

## 2.3: Simple Random Sampling

- **Total**: $\hat{\tau}$ is used to estimate $\tau$,

$$
\hat{\tau} = \frac{N}{n} \sum_{i =1}^n y_i = N \bar{y}
$$

- **Variance of the estimated total**: measures the variability among estimates of $\tau$ from different samples

$$
\hat{\text{var}}\left[ \hat{\tau} \right] = N^2\left(1-\frac{n}{N}\right) \frac{s^2}{n} = N^2 \hat{\text{var}}\left[ \bar{y} \right]
$$


## 2.3: Simple Random Sampling

- **Proportion**: we use $\hat{p}$ to estimate $p$.

    - First, let $y_i$ = 1 if the unit has the characteristic of interest and $y_i$ = 0 if not. 
    
    - Then, we estimate the population proportion with
    
$$
\hat{p} = \frac{1}{n} \sum_{i=1}^n y_i
$$

- i.e., the proportion is an *average*.

- **Variance of the estimated proportion**: measures the variability among estimates of $p$ from different samples

$$
\hat{\text{var}}\left[ \hat{p} \right] = \left( 1 - \frac{n}{N} \right) \frac{\hat{p} \left(1-\hat{p}\right)}{n-1}
$$

## 2.3: Simple Random Sampling

- Variance is great because it tells us about the spread... but it's in squared units.

    - We are interested in discussing the spread in terms of the original units.

- For any estimated variance, we can find the **standard error (SE)**:

$$
\text{SE}\left[\hat{\theta}\right] = \sqrt{ \hat{\text{var}}\left[ \hat{\theta} \right]}
$$

- **Coefficient of variation**: This adjusts the spread relative to the estimated parameter,

$$
\hat{\text{CV}} = \frac{\text{SE}\left[\hat{\theta}\right]}{\hat{\theta}}
$$

## 2.3: Simple Random Sampling

- Let us now review how to find these statistics in R.

- Finding the mean ($\bar{y}$) and the variance ($s^2$) using [`summarize()`](https://dplyr.tidyverse.org/reference/summarise.html) from [`dplyr`](https://dplyr.tidyverse.org/)/[`tidyverse`](https://www.tidyverse.org/):

```{r, echo = TRUE}
agsamp %>% summarize( # request summary statistics
  mean = mean(acres92, na.rm = TRUE), # request the mean
  var = var(acres92, na.rm = TRUE) # request the variance
  )  # note: the na.rm = TRUE excludes missing from calculation
```

## 2.3: Simple Random Sampling

- But... we are interested in the estimated variance of $\bar{y}$, SE, and CV.

```{r, echo = TRUE}
# set values to use in computation:
m = mean(agsamp$acres92, na.rm = TRUE) # find the mean of acres92
s2 = var(agsamp$acres92, na.rm = TRUE) # find the variance of acres92
n = nrow(agsamp) # count the number of rows for sample size
N = nrow(agpop) # count the number of rows for population size

# computations:
estvar = (1-n/N)*s2/n # find the estimated variance
SE = sqrt(estvar) # find the SE
CV = SE/m
```

- $\hat{\text{var}}\left[ \bar{y} \right] = `r format(estvar, scientific = FALSE)`$ 

- $\hat{\text{SE}}\left[ \bar{y} \right] = `r format(SE, scientific = FALSE)`$

- $\hat{\text{CV}}\left[ \bar{y} \right] = `r round(CV,4)`$

## 2.3: Simple Random Sampling

- Suppose we are also interested in the proportion of counties with more than 200,000 acres in farms.

- We need to first construct an indicator variable for acreage.

```{r, echo = TRUE}
agsamp <- agsamp %>% mutate(over200k = ifelse(acres92 < 200000, 1, 0))
```

- Then we can count,

```{r, echo = TRUE}
agsamp %>% count(over200k)
```

## 2.3: Simple Random Sampling

- To find the statistics we are interested in,

```{r, echo = TRUE}
# set values to use in computation:
sum_y = sum(agsamp$over200k, na.rm = TRUE) # count the number with the characteristic
n = nrow(agsamp) # count the number of rows for sample size
N = nrow(agpop) # count the number of rows for population size

# computations:
p = sum_y/n # find the proportion
estvar = (1-n/N)*(p*(1-p))/n # find the estimated variance
SE = sqrt(estvar) # find the SE
```

- $\hat{p} = `r round(p, 4)`$

- $\hat{\text{var}}\left[ \bar{y} \right] = `r format(round(estvar, 4), scientific = FALSE)`$ 

- $\hat{\text{SE}}\left[ \bar{y} \right] = `r round(SE, 4)`$

## 2.4: Sampling Weights

- Recall that we defined $\pi_i$ to be the probability that unit $i$ is included in the sample.

    - These inclusion probabilities will be used in the calculation of point estimates.
    
- **Sampling weight**: the number of population units represented by unit $i$.

$$
w_i = \frac{1}{\pi_i}
$$

- In simple random sampling, all units have equal inclusion probability, thus, equal sampling weights.

    - This type of sample is called a **self-weighting sample**.

$$
\pi_i = \frac{n}{N} \text{ and } w_i = \frac{1}{\pi_i} = \frac{N}{n}
$$

## 2.5: Confidence Intervals

- **Confidence interval** (CI): the interval estimator of a parameter, indicative of the accuracy of the point estimate.

- Consider a 95% CI.

    - The 95% is the coverage probability.

    - If we repeatedly take samples from the population and construct a CI from each sample, we expect that 95% of them contain the true value of the population parameter.

- Much of statistics relies on asymptotic results (i.e., assume that $N \to \infty$).

    - We are technically sampling from finite populations, but we assume that our population is part of a larger superpopulation.
    
    - This assumption allows us to use properties of asymptotic normality.

## 2.5: Confidence Intervals

- Our confidence intervals will take the form point estimate $\pm$ margin of error, where the margin of error is the critical value $\times$ standard error.

- In the case of the mean,

$$ 
\bar{y} \pm z_{\alpha/2} \text{SE}\left[ \bar{y} \right]
$$

- In the case of the total,

$$ 
\hat{\tau} \pm z_{\alpha/2} \text{SE}\left[ \hat{\tau} \right]
$$

- In the case of the proportion,

$$ 
\hat{p} \pm z_{\alpha/2} \text{SE}\left[ \hat{p} \right]
$$

- Note: R uses the *t* distribution in place of the *z* distribution when estimating $\mu$ and $\tau$.

## 2.5: Confidence Intervals

- We can use the [`t.test()` function](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/t.test) to find confidence intervals for $\mu$.

```{r, echo = TRUE}
t.test(agsamp$acres92, conf.level = 0.95)
```

- $\bar{y} = 336579.1$

- The 95% CI for $\mu$ is $(272429.4, 400528.8)$.

## 2.5: Confidence Intervals

- We will construct the CI for $\tau$ by "hand".

- We will need to find the critical value of *t* using the [`qt()` function](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/TDist).
    
    - We need to specify $1-\alpha/2$ and the degrees of freedom, $n-1$.

```{r, echo = TRUE}
# set values to use in computation:
m = mean(agsamp$acres92, na.rm = TRUE) # find the mean of acres92
s2 = var(agsamp$acres92, na.rm = TRUE) # find the variance of acres92
n = nrow(agsamp) # count the number of rows for sample size
N = nrow(agpop) # count the number of rows for population size
t = qt(.975, n-1)

# computations:
tau_hat = N*m
estvar = N^2*(1-n/N)*s2/n # find the estimated variance
SE = sqrt(estvar) # find the SE
lb = tau_hat - t*SE
ub = tau_hat + t*SE
```

- $\hat{\tau} = `r format(tau_hat, scientific = FALSE)`$

- The 95% CI for $\tau$ is $(`r format(lb, scientific = FALSE)`, `r format(ub, scientific = FALSE)`)$.

## 2.5: Confidence Intervals

- We can use the [`prop.test()` function](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/prop.test) to find confidence intervals for $p$.

```{r, echo = TRUE}
prop.test(sum(agsamp$over200k, na.rm = TRUE), n=nrow(agsamp))
```

- $\hat{p} = 0.4967$

- The 95% CI for $p$ is $(0.4388, 0.5546)$.

## 2.7: Determining the Sample Size

- Before executing a study/survey, a sample size should be determined.

    - We need to know what amount of sampling error is "okay" and balance the precision of the estimates with the cost of the experiment/survey.
    
- Although we collect as much relevant information as possible from sampling units, there are going to be one or two responses that are of primary interest.

    - These are what we use to determine the sample size necessary.
    
- When I am asked for a sample size, my first response is, "How many can you afford?"

    - The textbook notes that if you are new at sample size estimation, you will likely find that the sample size needed is much larger than can be afforded.
    
## 2.7: Determining the Sample Size

- Our first step is to specify the tolerable error -- this can only be defined by the researcher. Mathematically,

$$ 
\text{P} \left[ |\bar{y}-\mu | \le e \right] = 1 - \alpha
$$

- where $e$ is the margin of error and is one-half the width of the CI
    
- Note that while we can specify any $\alpha \in (0, 1)$, we most often specify $\alpha=0.05$.

    - Soapbox about $\alpha=0.05$ ðŸ˜¬

## 2.7: Determining the Sample Size

- Then, we find an equation relating the precision and sample size and solve for $n$.
    
- Using the margin of error from previous CIs,

$$
\begin{align*}
e &= z_{\alpha/2} \sqrt{\left( 1 - \frac{n}{N} \right)} \frac{S}{\sqrt{n}} \\
n_0 & = \left( \frac{z_{\alpha/2}S}{e} \right)^2
\end{align*}
$$

- If we need to apply the finite population correction (FPC),

$$
n = \frac{n_0}{1+\frac{n_0}{N}} = \frac{z_{\alpha/2}^2 S^2}{e^2 + \frac{z_{\alpha/2}^2 S^2}{N}}
$$

## 2.7: Determining the Sample Size

- Suppose we want to estimate the proportion of recipes in a cookbook that do not involve animal products. A simple random sample of the 1251 recipes in the cookbook will be drawn. We want to construct a 95% CI with margin of error 0.03.

    - Because we do not have an estimate for $\hat{p}$, we will plug in $\hat{p}=0.50$.

$$
\begin{align*}
n_0 &= \left( \frac{z_{\alpha/2}S}{e} \right)^2 \\
&= \frac{1.96^2 (0.5)(1-0.5)}{0.03^2} \\
&= 1067.111 \\
&\approx 1068
\end{align*}
$$

- Note 1: we almost always round up when estimating sample size.

    - Note 1a: except when dealing with cost restrictions.
    
- Note 2: 1068/1251 = needing a sample size that is 85% of the population ðŸ˜±  
    
## 2.7: Determining the Sample Size

- We do need to apply the FPC because 1068/1251 is a large proportion.

$$
\begin{align*}
n &= \frac{n_0}{1+\frac{n_0}{N}} \\
&= \frac{1067}{1+\frac{1067}{1251}} \\
&= 576.1397 \\
&\approx 577
\end{align*}
$$

- Thus, the sample size of $n=577$ will ensure a maximum margin of error of 0.03 when estimating this proportion.

    - Note that it will be smaller if $\hat{p} \ne 0.5$.
    
    - By plugging in $\hat{p} = 0.5$, we are being conservative with our estimation.
    
## 2.7: Determining the Sample Size

- Recall that we took a sample of 300 from the **agpop** dataset.

- Using a pilot sample of 30 found the sample standard deviation to be 519,085 acres.

- If we want a margin of error of 60,000 acres, then

$$
\begin{align*}
n_0 &= \left( \frac{z_{\alpha/2}S}{e} \right)^2 \\
&= \frac{1.96^2 (519085)^2}{60000^2} \\
&= `r round(1.96^2*519085^2/(60000^2), 4)` \\
&\approx 288
\end{align*}
$$

- The sample size was then to 300 to ensure that we would reach 288 in the event of missing data.

    - Always keep in mind that the sample size found is the *minimum* needed to hit the margin of error.

## Homework

1, 2, 3, 4, 5, 6, 7, 12, 13, 15, 16, 17, 18, 31


